---
seed_everything: 42

model:
  class_path: multimodal_rssm.models.mrssm.mopoe_mrssm.MoPoE_MRSSM
  init_args:
    audio_representation:
      class_path: multimodal_rssm.models.networks.Representation
      init_args:
        deterministic_size: 32
        hidden_size: 32
        obs_embed_size: 64
        distribution_config: [4, 4]
        activation_name: ELU
    vision_representation:
      class_path: multimodal_rssm.models.networks.Representation
      init_args:
        deterministic_size: 32
        hidden_size: 32
        obs_embed_size: 64
        distribution_config: [4, 4]
        activation_name: ELU
    transition:
      class_path: multimodal_rssm.models.networks.Transition
      init_args:
        deterministic_size: 32
        hidden_size: 32
        action_size: 6
        distribution_config: [4, 4]
        activation_name: ELU
    audio_encoder:
      class_path: cnn.Encoder
      init_args:
        config:
          linear_sizes: [64,]
          activation_name: ELU
          out_activation_name: Identity
          channels: [8, 16, 32]
          kernel_sizes: [3, 3, 3]
          strides: [2, 2, 2]
          paddings: [1, 1, 1]
          num_residual_blocks: 3
          residual_intermediate_size: 64
          residual_output_size: 64
          coord_conv: true
    vision_encoder:
      class_path: cnn.Encoder
      init_args:
        config:
          linear_sizes: [64,]
          activation_name: ELU
          out_activation_name: Identity
          channels: [8, 16, 32]
          kernel_sizes: [3, 3, 3]
          strides: [2, 2, 2]
          paddings: [1, 1, 1]
          num_residual_blocks: 3
          residual_intermediate_size: 64
          residual_output_size: 64
          coord_conv: true
    audio_decoder:
      class_path: cnn.Decoder
      init_args:
        config:
          linear_sizes: [64, 1024]
          conv_in_shape: [64, 4, 4]
          activation_name: ELU
          out_activation_name: Tanh
          channels: [32, 16, 1]
          kernel_sizes: [4, 4, 4]
          strides: [2, 2, 2]
          paddings: [1, 1, 1]
          output_paddings: [0, 0, 0]
          num_residual_blocks: 3
          residual_intermediate_size: 128
          residual_input_size: 64
    vision_decoder:
      class_path: cnn.Decoder
      init_args:
        config:
          linear_sizes: [64, 1024]
          conv_in_shape: [64, 4, 4]
          activation_name: ELU
          out_activation_name: Tanh
          channels: [32, 16, 1]
          kernel_sizes: [4, 4, 4]
          strides: [2, 2, 2]
          paddings: [1, 1, 1]
          output_paddings: [0, 0, 0]
          num_residual_blocks: 3
          residual_intermediate_size: 128
          residual_input_size: 64
    init_proj:
      class_path: torchrl.modules.MLP
      init_args:
        in_features: 64
        out_features: 32
        num_cells: 200
        depth: 1
    kl_coeff: 1
    use_kl_balancing: true

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.001

lr_scheduler:
  class_path: lightning.pytorch.cli.ReduceLROnPlateau
  init_args:
    monitor: val/loss
    mode: min
    factor: 0.5
    patience: 50

trainer:
  accelerator: gpu
  max_epochs: 100
  gradient_clip_val: 10
  deterministic: true
  precision: 16-mixed
  log_every_n_steps: 1

  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      log_model: false
      project: mopoe_mrssm
      save_dir: .venv

  callbacks:
    -
      class_path: multimodal_rssm.models.callback.WandBMetricOrganizer
    -
      class_path: LearningRateMonitor
    -
      class_path: EarlyStopping
      init_args:
        monitor: val/loss
        patience: 200
        mode: min
        verbose: True
    -
      class_path: ModelCheckpoint
      init_args:
        monitor: val/loss
        mode: min
        save_top_k: 1
    -
      class_path: multimodal_rssm.models.mrssm.mopoe_mrssm.callback.LogMoPoEMRSSMOutput
      init_args:
        every_n_epochs: 10
        indices: [0, 1, 2]
        query_length: 10
        fps: 10.0

data:
  class_path: multimodal_rssm.models.mrssm.dataset.EpisodeDataModule
  init_args:
    config:
      data_name: audio_mnist
      batch_size: 8
      num_workers: 4
      gdrive_url: ""
      audio_observation_file_name: ""
      vision_observation_file_name: ""
      action_preprocess:
        class_path: torch.nn.Identity
      audio_observation_preprocess:
        class_path: multimodal_rssm.models.transform.NormalizeAudioMelSpectrogram
        init_args:
          min_value: -80.0
          max_value: 0.0
      vision_observation_preprocess:
        class_path: multimodal_rssm.models.transform.NormalizeVisionImage
      action_input_transform:
        class_path: torchvision.transforms.Compose
        init_args:
          transforms:
            - class_path: multimodal_rssm.models.transform.TakeFirstN
              init_args:
                n: 30
            - class_path: multimodal_rssm.models.transform.GaussianNoise
      action_target_transform:
        class_path: torchvision.transforms.Compose
        init_args:
          transforms:
            - class_path: multimodal_rssm.models.transform.TakeFirstN
              init_args:
                n: 30
      audio_observation_input_transform:
        class_path: torchvision.transforms.Compose
        init_args:
          transforms:
            - class_path: multimodal_rssm.models.transform.TakeFirstN
              init_args:
                n: 30
            - class_path: multimodal_rssm.models.transform.GaussianNoise
      audio_observation_target_transform:
        class_path: torchvision.transforms.Compose
        init_args:
          transforms:
            - class_path: multimodal_rssm.models.transform.TakeFirstN
              init_args:
                n: 30
      vision_observation_input_transform:
        class_path: torchvision.transforms.Compose
        init_args:
          transforms:
            - class_path: multimodal_rssm.models.transform.TakeFirstN
              init_args:
                n: 30
            - class_path: multimodal_rssm.models.transform.GaussianNoise
      vision_observation_target_transform:
        class_path: torchvision.transforms.Compose
        init_args:
          transforms:
            - class_path: multimodal_rssm.models.transform.TakeFirstN
              init_args:
                n: 30
